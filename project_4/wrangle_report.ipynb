{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Wrangling</b> has 3 steps: gather, assess, clean. These steps can iterate as new tasks appear. You should gather additional data (in some cases you can't) if you don't have enough for your analysis. \n",
    "<br>In my opinion, the hardest task is to ask right questions. When you assess data you have to find missing values, incorrect values, multiple representations (duplicates), sometimes you don't have data for some observations and you have to gather again. In this case, we don't have enough data such as: retweet count and favorite count. So, we gathered this data through Twitter API called Tweepy. We got required data but 19 observations have missing values. It happened because some tweets are deleted and we can't do anything.<br>\n",
    "We saw that the data have incorrect(invalid) observations, in our case: retweets and replies to tweets. Fortunately, we can easy determine them by in_reply_to_status_id (replies) and retweeted_status_id (retweets). There was only one solution to this problem, as we were interested only in original tweets, we removed these observations. <br>\n",
    "Another problem was that 3 observations don't have expanded urls, these observations don't have images,  only notifications about something. We removed these invalid values from the data.<br>\n",
    "3 main problems of the data were: rating numerator, rating denominator and names of the dogs.<br>\n",
    "Firstly, denominator, according to their rate rules, should be 10 but we saw a lot of observations which don't support that rule.<br>\n",
    "Numerator should be less than 14, 14 is the highest rating. \n",
    "I checked all values with numerator greater than 14 and denominator not equal 10, all these observations have images where more than 1 dog or even a human(Snoop Dog), I decided to remove these rows as we can't compare them to the other tweets.<br> \n",
    "And the names have invalid values such as: 'a', 'an', 'old', etc.<br>\n",
    "Some values we could find from the text of each observation but in most cases we just don't have the required data of dog names.<br>\n",
    "Another problem was predictions of dog breeds. More than 300 values of prediction labels are incorrect. The problem was that there was no pattern of these incorrect values. So, you have to manually find these values. Of course, if you had access to the machine learning algorithm and change these values, it would be easy to handle the problem at the beginning.<br>\n",
    "Another problem was duplicates in the image predictions. Tweet id's don't have duplicates but jpg_url's do. 66 duplicares of jpg_url's, so what does it mean? I checked these tweet id's of duplicates and found out that these duplicates had 'retweeted_status_id' from the first data set (twitter_archive), so, we removed these duplicates.<br>\n",
    "Another issue was representation 2 variables in the same column. We extracted url from the text and created a new column 'quick_url'.<br>\n",
    "We also created a new column 'stage' and remove 4 columns: doggo, puppo, floofer, pupper.<br>\n",
    "At the end we created a new data set which contains image predictions and tweet scores. Of course, we could join 3 tables toghether, but in this case I decided to separate them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
